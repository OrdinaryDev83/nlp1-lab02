{"cells":[{"cell_type":"markdown","source":"# Introduction to NLP 01 - LAB 02\n## The Dataset","metadata":{"cell_id":"ae87597f90604a03ac1ec0e6f77bdf80","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"from datasets import load_dataset\nimport pandas as pd\nimport numpy as np\n\nX_train, X_test, X_unsupervised = load_dataset(\"imdb\", split=[\"train\", \"test\", \"unsupervised\"])\nprint(X_train.shape, X_test.shape, X_unsupervised.shape)","metadata":{"cell_id":"79d53b2df5704620aadd8983c205459c","source_hash":"28d63c42","execution_start":1680109772644,"execution_millis":2184,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"/shared-libs/python3.9/py/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nFound cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/d613c88cf8fa3bab83b4ded3713f1f74830d1100e171db75bbddb80b3345c9c0)\n100%|██████████| 3/3 [00:00<00:00, 306.05it/s](25000, 2) (25000, 2) (50000, 2)\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df_train, df_test, df_unsupervised = pd.DataFrame(X_train), pd.DataFrame(X_test), pd.DataFrame(X_unsupervised)","metadata":{"cell_id":"1aaaecbfcd114224836d7e1b8b771ad8","source_hash":"858dc98f","execution_start":1680109774893,"execution_millis":6999,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":2},{"cell_type":"code","source":"df_train['label'].value_counts()","metadata":{"cell_id":"84b11e3b74c740dd84c48f22b93255aa","source_hash":"7794f20e","execution_start":1680109781895,"execution_millis":47,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"0    12500\n1    12500\nName: label, dtype: int64"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"df_test['label'].value_counts()","metadata":{"cell_id":"ea8e172cb5c14c9290e79aa5c7a1a0cb","source_hash":"a2b5a8d0","execution_start":1680109781940,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"0    12500\n1    12500\nName: label, dtype: int64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"df_unsupervised['label'].value_counts()","metadata":{"cell_id":"7521722c4e4c457bacbcf9be3c8e594a","source_hash":"a15dd194","execution_start":1680109781941,"execution_millis":48,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"-1    50000\nName: label, dtype: int64"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"\n1. How many splits does the dataset has?  \n    There are **3 splits** (train, test, unsupervised) in the dataset.  \n2. How big are these splits? \n    - *train* : **25000** samples in total, **12500** positive (1) and **12500** negative (0) samples.\n    - *test* : same on test but with different samples\n    - *unsupervised* : **50000** samples in total with no labels (-1)\n3. What is the proportion of each class on the supervised splits?\n    - The proportion of each on train and test supervised splits is **50/50** (12500 for positive label and 12500)\n","metadata":{"cell_id":"7cda791248d94bd88675bee62af2f5b9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Naive Bayes Classifier","metadata":{"cell_id":"8115b9ac3237482ea9685fde58d25ea0","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Implement your own naive Bayes classifier (the pseudo code can be found in the slides or the [book reference](https://web.stanford.edu/~jurafsky/slp3/)) or use [one provided by scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes) combined with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n\nGo through the following steps.\n1. (2 points) Take a look at the data and create an adapted preprocessing function with at least:\n   i. Lower case the text.\n   ii. Replace punctuations with spaces (you can use `from string import punctuation` to ease your work). Think that maybe not all punctuations should be removed or replaced.","metadata":{"cell_id":"9bf24ba481ee4c07844132a98159b80c","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"df_train.head()","metadata":{"cell_id":"4b1934d527894b43adbbde555b706cc7","source_hash":"5e2a9c26","execution_start":1680109781996,"execution_millis":24,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"application/vnd.deepnote.dataframe.v3+json":{"column_count":2,"row_count":5,"columns":[{"name":"text","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn't have much of a plot.","count":1},{"name":"\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won't see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women's bodies.","count":1},{"name":"3 others","count":3}]}},{"name":"label","dtype":"int64","stats":{"unique_count":1,"nan_count":0,"min":"0","max":"0","histogram":[{"bin_start":-0.5,"bin_end":-0.4,"count":0},{"bin_start":-0.4,"bin_end":-0.3,"count":0},{"bin_start":-0.3,"bin_end":-0.19999999999999996,"count":0},{"bin_start":-0.19999999999999996,"bin_end":-0.09999999999999998,"count":0},{"bin_start":-0.09999999999999998,"bin_end":0,"count":0},{"bin_start":0,"bin_end":0.10000000000000009,"count":5},{"bin_start":0.10000000000000009,"bin_end":0.20000000000000007,"count":0},{"bin_start":0.20000000000000007,"bin_end":0.30000000000000004,"count":0},{"bin_start":0.30000000000000004,"bin_end":0.4,"count":0},{"bin_start":0.4,"bin_end":0.5,"count":0}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows":[{"text":"I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, ev…","label":"0","_deepnote_index_column":"0"},{"text":"\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn't matter what one's political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn't true. I've seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don't exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we're treated to the site of Vincent Gallo's throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same can…","label":"0","_deepnote_index_column":"1"},{"text":"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />","label":"0","_deepnote_index_column":"2"},{"text":"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.","label":"0","_deepnote_index_column":"3"},{"text":"Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was,…","label":"0","_deepnote_index_column":"4"}]},"text/plain":"                                                text  label\n0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n1  \"I Am Curious: Yellow\" is a risible and preten...      0\n2  If only to avoid making this type of film in t...      0\n3  This film was probably inspired by Godard's Ma...      0\n4  Oh, brother...after hearing about this ridicul...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If only to avoid making this type of film in t...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>This film was probably inspired by Godard's Ma...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Oh, brother...after hearing about this ridicul...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from string import punctuation\nimport re\n\ndef preprocessing(data : str) -> str:\n    \"\"\"\n    Preprocess the data by removing punctuation, converting to lowercase and removing extra spaces\n    The function does not remove the \"-\" dash symbol\n\n    For example : son-in-law ≠ son in law\n\n    :param data: string data to preprocess \n    :return: preprocessed string data\n    \"\"\"\n    data = data.replace(\"<br />\", \"\")\n    toremove = punctuation.replace(\"-\", \"\")\n    translator = str.maketrans(toremove, ' ' * len(toremove))\n    res = data.lower().translate(translator).strip()\n    res = re.sub(r'\\s+', ' ', res)\n    return res","metadata":{"cell_id":"6ed60e11c62c49e286beeb87a6f581e7","source_hash":"a8eb652","execution_start":1680109782027,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"Example on one piece of text","metadata":{"cell_id":"015fd87e242846be94f771ae13936a2c","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"preprocessing(df_train.iloc[0]['text'])","metadata":{"cell_id":"f44ee11fd624433a8b12f7fbb70a1e3e","source_hash":"2dc0a99f","execution_start":1680109782093,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"'i rented i am curious-yellow from my video store because of all the controversy that surrounded it when it was first released in 1967 i also heard that at first it was seized by u s customs if it ever tried to enter this country therefore being a fan of films considered controversial i really had to see this for myself the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states in between asking politicians and ordinary denizens of stockholm about their opinions on politics she has sex with her drama teacher classmates and married men what kills me about i am curious-yellow is that 40 years ago this was considered pornographic really the sex and nudity scenes are few and far between even then it s not shot like some cheaply made porno while my countrymen mind find it shocking in reality sex and nudity are a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford had sex scenes in his films i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america i am curious-yellow is a good film for anyone wanting to study the meat and potatoes no pun intended of swedish cinema but really this film doesn t have much of a plot'"},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"We then apply it to the train dataset.","metadata":{"cell_id":"23171bc677ee47ab91e4153ce0a9418d","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"df_train[\"text\"] = df_train[\"text\"].apply(lambda x: preprocessing(x))","metadata":{"tags":[],"cell_id":"1ed55d13ddaf47eeb4ce18573f5c95d0","source_hash":"1f2a198f","execution_start":1680109782127,"execution_millis":6667,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"2. (4 points) Implement your own naive Bayes classifier from scratch. The pseudo code can be found in the slides or the [book reference](https://web.stanford.edu/~jurafsky/slp3/).","metadata":{"cell_id":"350498263a2d4dc5a14adecd35a8bb9e","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def get_count(dataset: pd.DataFrame, binary_class : str) -> pd.DataFrame:\n    \"\"\"\n    Get the count of each word in a dataset for a specific class\n    :param dataset: dataset to extract the words from\n    :param binary_class: the class to extract\n    :return: dataframe with the count of each word\n    \"\"\"\n    return dataset[dataset[\"label\"] == binary_class]['text'].str.split(expand=True).stack().value_counts()","metadata":{"cell_id":"91bc5abbcacd4d23b5b5b5263ea39586","source_hash":"20446cd0","execution_start":1680109788795,"execution_millis":36,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def get_words(dataset: pd.DataFrame) -> list[str]:\n    \"\"\"\n    Get all distinct words from dataset\n    :param dataset: pd.DataFrame containing the text column\n    :return: list of distinct string words from the corpus\n    \"\"\"\n    return list(set(' '.join([word for word in dataset['text']]).split()))","metadata":{"tags":[],"cell_id":"c7a157b73fef4f00906d56833b1f133f","source_hash":"a23966ad","execution_start":1680109788829,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def get_words_of_class(dataset: pd.DataFrame, binary_class: int) -> list[str]:\n    \"\"\"\n    Get all distinct words from dataset of with a specific label\n    :param dataset: pd.DataFrame containing the text and label columns\n    :param binary_class: int representing the class to extract\n    :return: list of distinct string with the specified label\n    \"\"\"\n    dataset = dataset[dataset[\"label\"] == binary_class]\n    return get_words(dataset)","metadata":{"tags":[],"cell_id":"48f48c3fb3ae45d9b140cba50d53f9f6","source_hash":"17e85d64","execution_start":1680109788830,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def train_naive_bayes(dataset: pd.DataFrame, classes: list[int]) -> (float, float, list[str]):\n    \"\"\"\n    Train a Naive Bayes model based on the algorithms described in the slides and in https://web.stanford.edu/~jurafsky/slp3/4.pdf\n    :param dataset: pd.DataFrame containing the text and label columns\n    :param classes: list of int representing the binary classes (0 Negative and 1 Positive)\n    :return: A tuple containing (logprior as a float, loglikelihood as a float, vocabulary as a list of distinct string)\n    \"\"\"\n    vocabulary = []\n    logprior = {}\n    loglikelihood = {}\n    counts = {}\n\n    for binary_class in classes:\n        Ndoc = len(dataset)\n        Nc = dataset[dataset[\"label\"] == binary_class].shape[0]\n        logprior[binary_class] = np.log(Nc / Ndoc)\n        vocabulary = get_words(dataset)\n\n        \n        counts[binary_class] = get_count(dataset, binary_class)\n        total_count = counts[binary_class].sum() + counts[binary_class].shape[0]\n\n        for word in vocabulary:\n            if not (word in counts[binary_class]):\n                count = 0\n            else:\n                count = counts[binary_class][word]\n            loglikelihood[(word, binary_class)] = np.log((count + 1) / total_count)\n\n    return logprior, loglikelihood, vocabulary","metadata":{"tags":[],"cell_id":"80cfebb1ccb346a0a99388722b55ac9d","source_hash":"a00fbfaf","execution_start":1680109788831,"execution_millis":62,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def test_naive_bayes(testdoc : str, logprior : float, loglikelihood : float, classes : list[int], vocabulary : set[str]) -> int:\n    \"\"\"\n    Predict the label for the testdoc_words according to the train Naive Bayes model\n    :param testdoc_words: Target document string to predict the label\n    :param logprior: Logprior of the Naive Bayes model\n    :param loglikelihood: Loglikelihood of the Naive Bayes model\n    :param classes: list of int representing the binary classes (0 Negative and 1 Positive)\n    :param vocabulary: list of distinct string representing the vocabulary\n    :return: int representing the predicted label\n    \"\"\"\n    testdoc_words = testdoc.split(\" \")\n    \n    confidence_per_class = {}\n    \n    for binary_class in classes:\n        confidence_per_class[binary_class] = logprior[binary_class]\n        \n        for i in range(len(testdoc_words)):\n            word = testdoc_words[i]\n            if word in vocabulary:\n                confidence_per_class[binary_class] += loglikelihood[(word, binary_class)]\n    \n    return np.argmax(list(confidence_per_class.values()))","metadata":{"tags":[],"cell_id":"efe47d428705406fbecba2b716fbe84e","source_hash":"2555e26c","execution_start":1680109788900,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"3. (3 points) Implement a naive Bayes classifier using scikit-learn.\n   * Use a scikit-learn [Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) with a [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) classifier. You can use other ","metadata":{"cell_id":"8d71bfc12c8e4525b7f4d593b667fff6","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"X_train, y_train = df_train['text'], df_train['label']","metadata":{"cell_id":"0d2efe58fc3647c99fccb016971c193e","source_hash":"b1a52a80","execution_start":1680109788904,"execution_millis":21,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\n\npipeline = Pipeline([\n    ('vectorizer', CountVectorizer()), \n    ('multiniomialNB', MultinomialNB())\n])","metadata":{"tags":[],"cell_id":"15bd089740484aae95e920b336c5ad30","source_hash":"91216eac","execution_start":1680109788926,"execution_millis":1526,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"4. (1 point) Report the accuracy on both training and test set, for both your implementation and the scikit-learn one.","metadata":{"cell_id":"9c609ee003524cafbaecebde08a1f5c7","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score","metadata":{"cell_id":"57061f695cf64c7ca3e8e52302fcd15e","source_hash":"dcc0610b","execution_start":1680109790460,"execution_millis":75,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":17},{"cell_type":"code","source":"X_test, y_test = df_test.text.apply(lambda x: preprocessing(x)), df_test['label']","metadata":{"cell_id":"3b4fe1fac6e746fea833f68948dabc65","source_hash":"431299ef","execution_start":1680109790578,"execution_millis":6467,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"Training the home-made naive Bayes classifier","metadata":{"cell_id":"282dbb3a59ac48c18a45053e49006302","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"*We need to convert the list vocabulary to a set in order to get a $O(1)$ complexity on element existence*","metadata":{"cell_id":"8a6201d39dac4bb79d811f8be53222e1","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"logprior, loglikelihood, vocabulary = train_naive_bayes(df_train, [0, 1])\nvocabulary = set(vocabulary)","metadata":{"cell_id":"bad592f69de648c1a094ea72262aa2e3","source_hash":"ec3fa055","execution_start":1680109797098,"execution_millis":42393,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"Testing it on the Train & Test datasets\n","metadata":{"cell_id":"60e23cf78d8b49f7bbc0a1bb6af8e078","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"results_train = [test_naive_bayes(text, logprior, loglikelihood, [0, 1], vocabulary) for text in X_train]","metadata":{"cell_id":"475f4d4ef9f245b4951c991c0e947372","source_hash":"c0d35e66","execution_start":1680109839503,"execution_millis":15920,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":20},{"cell_type":"code","source":"results_test = [test_naive_bayes(text, logprior, loglikelihood, [0, 1], vocabulary) for text in X_test]","metadata":{"cell_id":"2fdf92c0e43148c0b81648460929a681","source_hash":"38979c7c","execution_start":1680109855495,"execution_millis":15226,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":21},{"cell_type":"code","source":"print(f\"Our Naive Bayes accuracy : train {accuracy_score(y_train, results_train) * 100}% - test {accuracy_score(y_test, results_test) * 100}%\")","metadata":{"tags":[],"cell_id":"83470099d630463ba2a20e15d5e59339","source_hash":"d812d19a","execution_start":1680109870763,"execution_millis":38,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Our Naive Bayes accuracy : train 89.932% - test 81.0%\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"Training the Scikit Learn Bayes classifier and predicting on the test dataset","metadata":{"cell_id":"f052e1f8bf04426b849259d1a62a7f6f","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"pipeline.fit(X_train, y_train)","metadata":{"cell_id":"a9bcd98c17a64cfcbd89d592525d3704","source_hash":"3855a608","execution_start":1680109870806,"execution_millis":11537,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"Pipeline(steps=[('vectorizer', CountVectorizer()),\n                ('multiniomialNB', MultinomialNB())])","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n                (&#x27;multiniomialNB&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n                (&#x27;multiniomialNB&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"prediction_train = pipeline.predict(X_train)","metadata":{"cell_id":"b7ce4f9583c04fc7acf529b57fdaba1c","source_hash":"5b649568","execution_start":1680109882392,"execution_millis":10239,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":24},{"cell_type":"code","source":"prediction_test = pipeline.predict(X_test)","metadata":{"cell_id":"d9cee1b93a8f43b9aa2c2ab03b2de93a","source_hash":"43375197","execution_start":1680109892721,"execution_millis":10368,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":25},{"cell_type":"code","source":"print(f\"Scikit Learn Naive Bayes : train {accuracy_score(y_train, prediction_train) * 100}% - test {accuracy_score(y_test, prediction_test) * 100}%\")","metadata":{"cell_id":"ea189161a7d447d2b2c80402cb6a97bd","source_hash":"78051881","execution_start":1680109903092,"execution_millis":25,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Scikit Learn Naive Bayes : train 89.812% - test 81.44%\n","output_type":"stream"}],"execution_count":26},{"cell_type":"markdown","source":"5. (1 point) Most likely, the scikit-learn implementation will give better results. Looking at the documentation, explain why it could be the case.\n    The output between our home-mode Naive bayes classifier and Sklearn seams to have almost the same result, but sklearn implementation in faster in term of time and memory.\n    - Scikit-learn CountVectorizer uses sparse matrices to enhance computation time and space.\n    - Scikit-learn CountVectorizer uses another type of tokenizer with the following pattern ”```(?u)\\b\\w\\w+\\b```” in order to extract words. The vocabulary will be different.\n\n6. (1 point) Why is accuracy a sufficient measure of evaluation here?\n    \n    - ![img](https://media.discordapp.net/attachments/1069902292423286814/1088578203976740904/Calculation-of-Precision-Recall-and-Accuracy-in-the-confusion-matrix.png?width=1275&height=582)\n    - As we can see on this image, the accuracy is the overall Right Predicted values over All Possible values. We need to measure the model's predictive ability, both on true positives and true negatives equally, this is why accuracy is the best metric.","metadata":{"cell_id":"6e6a0a35d11342d2b3ade90cabb51d46","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"7. (1 point) Using one of the implementation, take at least 2 wrongly classified example from the test set and try explaining why the model failed.","metadata":{"cell_id":"42815205ff5b475fb4f1f37c6c08fd77","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def incorrect_prediction(data : pd.DataFrame, pred : pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Get the rows of the test set where the prediction is different from the label\n    :param data: pd.DataFrame containing the text and label columns\n    :param pred: Prediction of the model\n    :return: The rows of the test set where the prediction is different from the label\n    \"\"\"\n    real_target = data['label']\n    return data[real_target != pred]","metadata":{"tags":[],"cell_id":"338d2ac2672049e6b3ca7e3f071cb493","source_hash":"2042cee","execution_start":1680109903115,"execution_millis":3,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":27},{"cell_type":"code","source":"incorrect_prediction(df_test, prediction_test).iloc[8][\"text\"]","metadata":{"tags":[],"cell_id":"647d43b472794aeead53685fe99fd63e","source_hash":"e8b4d7d6","execution_start":1680109903116,"execution_millis":2,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":28,"data":{"text/plain":"'Talented screenwriter Alvin Sargent sadly cannot get any engaging ideas cooking in this artificial trifle about a wayward mother and her mature teenage daughter trying to make their lives work in Los Angeles despite mom\\'s flighty behavior. Apart from several good sequences, I didn\\'t quite buy Susan Sarandon as a flake (she\\'s too intrinsically smart and focused to be passed off as this devil-may-care lady), and her naturally grounded personality is a bad fit for the role of an irresponsible parent. Natalie Portman fares much better as her kid, and yet there\\'s a creepy aloofness to her work (and some of her scenes, such as the one where she asks a boy to strip, are misguided and uncomfortable to watch). Certainly not an incompetent piece, \"Anywhere But Here\" does have moments that work, but it isn\\'t an embraceable film, nor has it proved to be an important one. ** from ****'"},"metadata":{}}],"execution_count":28},{"cell_type":"markdown","source":"#### Incorrect classification 1:\n*\"The only reason this movie is not given a 1 (awful) vote is that the acting of both Ida Lupino and Robert Ryan is superb. Ida Lupino who is lovely, as usual, becomes increasingly distraught as she tries various means to rid herself of a madman. Robert Ryan is terrifying as the menacing stranger whose character, guided only by his disturbed mind, changes from one minute to the next. Seemingly simple and docile, suddenly he becomes clever and threatening. Ms. Lupino's character was in more danger from that house she lived in and her own stupidity than by anyone who came along. She could not manage to get out of her of her own house: windows didn't open, both front and back doors locked and unlocked from the inside with a key. You could not have designed a worse fire-trap if you tried. She did not take the precaution of having even one extra key. Nor could she figure out how to summon help from nearby neighbors or get out of her own basement while she was locked in and out of sight of her captor. I don't know what war her husband was killed in, but if it was World War II, the furnishings in her house, the styles of the clothes, especially the children and the telephone company repairman's car are clearly anachronistic. I recommend watching this movie just to see what oddities you can find.\"*\n\n#### Explanation (Classified as positive but was negative)\n\nThis text has been wrongly predicted to be positive as it has mixed tones for 3/4 of the review. Even though the overall review is negative, the author uses positive vocabulary on some very specific and rare elements of the movie, but the Naive Bayes algorithm can't understand context, and since the author talks about it for 3/4 of the review the algorithm predicts it as positive.\n\nFor example:\n - *\"the acting of both Ida Lupino and Robert Ryan is **superb**.\"* --> can be classified as positive\n - *\"Ida Lupino who is **lovely**, as usual\"* --> can be classified as positive\n\n---\n\n#### Incorrect classification 2:\n*'Talented screenwriter Alvin Sargent sadly cannot get any engaging ideas cooking in this artificial trifle about a wayward mother and her mature teenage daughter trying to make their lives work in Los Angeles despite mom\\'s flighty behavior. Apart from several good sequences, I didn\\'t quite buy Susan Sarandon as a flake (she\\'s too intrinsically smart and focused to be passed off as this devil-may-care lady), and her naturally grounded personality is a bad fit for the role of an irresponsible parent. Natalie Portman fares much better as her kid, and yet there\\'s a creepy aloofness to her work (and some of her scenes, such as the one where she asks a boy to strip, are misguided and uncomfortable to watch). Certainly not an incompetent piece, \"Anywhere But Here\" does have moments that work, but it isn\\'t an embraceable film, nor has it proved to be an important one.'*\n\n#### Explanation (Classified as positive but was negative)\n\nUsage of contracted negatition (n't) can lead to a bad interpretation due to the tokenization removing part of the information. The Naive Bayes classifier doesn't take context into account. \n- *\"**cannot** get any **engaging** ideas cooking\"* can be badly classified due to the negatition in the word cannot and engaging can be classified as positive","metadata":{"cell_id":"8cfb9e94dfb84d94b18e3a6bc145a8c6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### 8. Bonus","metadata":{"cell_id":"413b28cb3ffe4a96b88936ad9c4b5194","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h3"}},{"cell_type":"markdown","source":"1. Look at the words with the highest likelihood in each class (if you use scikit-learn, you want to check feature_log_prob_).","metadata":{"tags":[],"style":"lower-roman","number":1,"cell_id":"b039366a6d254169896cd4281e900c76","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-number"}},{"cell_type":"markdown","source":"2. Remove stopwords and check again.","metadata":{"tags":[],"style":"lower-roman","number":2,"cell_id":"0492153329704b71811bc05a2b8c9453","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-number"}},{"cell_type":"markdown","source":"To increase our Naive Bayes accuracy, we will add a set of stopwords to ignore which have a negligent impact on a text positiveness but which add noise being to much present in text.","metadata":{"tags":[],"cell_id":"b5a44b82f85045e2bd8053b92b656433","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def get_10_best(data : pd.DataFrame, binary_class : str) -> pd.Series:\n    \"\"\"\n    Get the 10 most frequent words in a dataset for a specific class\n    :param data: dataset to extract the words from\n    :param binary_class: label of the class to extract (Positive or Negative)\n    :return: The 10 most frequent words in a dataset for a specific class\n    \"\"\"\n    return pd.DataFrame(' '.join([i for i in data[data[\"label\"] == binary_class]['text']]).split()).value_counts().head(10)","metadata":{"cell_id":"babc879a760a48f2b8b2d40df19471ed","source_hash":"b0b467b6","execution_start":1680109903117,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":29},{"cell_type":"code","source":"get_10_best(df_train, 0)","metadata":{"cell_id":"8a55a486826045b48984f5ae8c5be866","source_hash":"8854b523","execution_start":1680109903190,"execution_millis":2407,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"text/plain":"the     162861\na        79138\nand      74068\nof       68794\nto       68718\nis       50033\nit       48201\ni        46781\nin       43441\nthis     40831\ndtype: int64"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"get_10_best(df_train, 1)","metadata":{"cell_id":"5db0c0fa7193453eabef43a8a8921355","source_hash":"10d98454","execution_start":1680109905603,"execution_millis":2428,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"the     172902\nand      89396\na        83498\nof       76638\nto       66476\nis       57208\nin       49858\nit       47879\ni        40642\nthat     35599\ndtype: int64"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"The 10 highset count of word of each classes are almost all only stopwords.","metadata":{"cell_id":"7095935550f9451e84eef5ca1f891ab0","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\n\nstopswords_nltk = set(stopwords.words('english'))","metadata":{"cell_id":"4f79dc0632114d2fad9a2ce5ea923806","source_hash":"f5c2ba8e","execution_start":1680109908092,"execution_millis":972,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"def get_10_best_without_stopword(data, binary_class, stopwords):\n    \"\"\"\n    Get the 10 most frequent words in a dataset for a specific class without stopwords\n    :param data: dataset to extract the words from\n    :param binary_class: label of the class to extract (Positive or Negative)\n    :param stopwords: list of stopwords to remove\n    :return: The 10 most frequent words in a dataset for a specific class without stopwords\n    \"\"\"\n    data = pd.DataFrame(' '.join([i for i in data[data[\"label\"] == binary_class]['text']]).split())\n    mask = data.isin(stopwords)  # mask in order to remove stopword\n\n    return data[~mask].value_counts().head(10)","metadata":{"cell_id":"afa8dcde03574f13b062981ef91c0013","source_hash":"f448e262","execution_start":1680109909036,"execution_millis":29,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":33},{"cell_type":"code","source":"get_10_best_without_stopword(df_train, 0, stopswords_nltk)","metadata":{"cell_id":"b7b4dc94636949669d5217522bcda3f4","source_hash":"cbca757d","execution_start":1680109909510,"execution_millis":2909,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"movie     24580\nfilm      18844\none       12734\nlike      10971\neven       7646\ngood       7285\nbad        7274\nwould      6988\nreally     6252\ntime       6001\ndtype: int64"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"get_10_best_without_stopword(df_train, 1, stopswords_nltk)","metadata":{"cell_id":"1de1643501f346c48658ec04abb5a96e","source_hash":"66adfbc6","execution_start":1680109912425,"execution_millis":3096,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"film     20623\nmovie    18856\none      13368\nlike      8790\ngood      7543\nstory     6674\ngreat     6384\ntime      6230\nsee       5896\nwell      5770\ndtype: int64"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"Without the stopwords some word are both present in the classes but as we expect *\"bad\"* is classified as negative. *\"Good\"* and *\"great\"* are classified as positive. We are positive that this removal would improve the model's accuracy.","metadata":{"cell_id":"b0ffd29d90b748e0ba8ad8736e57abe8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Remove stopwords (see NLTK stopwords corpus) and check again. ","metadata":{"cell_id":"7e3bb4ef00f5431cbdcb73d2f4041313","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"As NLTK's stopwords list contains word with apostrophe like \"doesn't\" or \"can't\", we will not use it because we deleted every apostrophes in our preprocessing.\nSo we will use a list of stopwords without apostrophe but with words \"don\" and \"t\" to replace \"don't\"","metadata":{"cell_id":"169248f29279498eb51853a82d64a360","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"stopwords_modified = {\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"}","metadata":{"tags":[],"cell_id":"7960a00d798f4d3787757fa8a74ce019","source_hash":"1edb1a61","execution_start":1680109915530,"execution_millis":65,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def test_naive_bayes_without_stopwords(testdoc : str, logprior : float, loglikelihood : float, classes : list[int], vocabulary : set[str], stopword : set[str]) -> int:\n    \"\"\"\n    Predict the label for the testdoc_words according to the train Naive Bayes model\n    :param testdoc_words: Target document string to predict the label\n    :param logprior: Logprior of the Naive Bayes model\n    :param loglikelihood: Loglikelihood of the Naive Bayes model\n    :param classes: list of int representing the binary classes (0 Negative and 1 Positive)\n    :param vocabulary: list of distinct string representing the vocabulary\n    :return: int representing the predicted label\n    :param stopword: list of stopwords\n    \"\"\"\n    testdoc_words = testdoc.split(\" \")\n    \n    confidence_per_class = {}\n   \n    for binary_class in classes:\n        confidence_per_class[binary_class] = logprior[binary_class]\n        \n        for i in range(len(testdoc_words)):\n            word = testdoc_words[i]\n            if (not (word in stopword)) and word in vocabulary:\n                confidence_per_class[binary_class] += loglikelihood[(word, binary_class)]\n    \n    return np.argmax(list(confidence_per_class.values()))","metadata":{"cell_id":"0a244f17ed7c43aab1369714db6c8717","source_hash":"3035a071","execution_start":1680109915639,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":37},{"cell_type":"code","source":"results_train_without_stopwords = [test_naive_bayes_without_stopwords(text, logprior, loglikelihood, [0, 1], vocabulary, stopwords_modified) for text in X_train]","metadata":{"cell_id":"a629e69768d445dabe49b94a043da464","source_hash":"8a3c1706","execution_start":1680109915795,"execution_millis":12406,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":38},{"cell_type":"code","source":"results_test_without_stopwords = [test_naive_bayes_without_stopwords(text, logprior, loglikelihood, [0, 1], vocabulary, stopwords_modified) for text in X_test]","metadata":{"cell_id":"bb3fe17016f54bc984722843504d34e9","source_hash":"772621b5","execution_start":1680109928298,"execution_millis":10891,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":39},{"cell_type":"code","source":"print(f\"Our Naive Bayes accuracy : train {accuracy_score(y_train, results_train_without_stopwords) * 100}% - test {accuracy_score(y_test, results_test_without_stopwords) * 100}%\")","metadata":{"cell_id":"b158a760ef804890a589bfbb1b7d22d9","source_hash":"78ae983b","execution_start":1680109939191,"execution_millis":16,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Our Naive Bayes accuracy : train 91.964% - test 83.204%\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"The accuracy was improved from 89.812% to 91.964% on the train split and from 81.44% to 83.204% on test split.","metadata":{"cell_id":"ab8b56fd77154c4d9affd287e3c954fc","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"We reproduce the stopword removal with scikit-learn.","metadata":{"cell_id":"febeaaf54a49447ab6c38982e6fdb672","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"pipeline_without_stopwords = Pipeline([\n    ('vectorizer', CountVectorizer(stop_words=stopswords_nltk)), \n    ('multiniomialNB', MultinomialNB())\n])","metadata":{"cell_id":"a545481b738842fd892a6224139d2dea","source_hash":"8d408296","execution_start":1680109939213,"execution_millis":5,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":41},{"cell_type":"code","source":"pipeline_without_stopwords.fit(X_train, y_train)","metadata":{"cell_id":"6ea242a4e88b43c9b2e9591a2d7f8b55","source_hash":"b9f7f161","execution_start":1680109939318,"execution_millis":9771,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"Pipeline(steps=[('vectorizer',\n                 CountVectorizer(stop_words={'a', 'about', 'above', 'after',\n                                             'again', 'against', 'ain', 'all',\n                                             'am', 'an', 'and', 'any', 'are',\n                                             'aren', \"aren't\", 'as', 'at', 'be',\n                                             'because', 'been', 'before',\n                                             'being', 'below', 'between',\n                                             'both', 'but', 'by', 'can',\n                                             'couldn', \"couldn't\", ...})),\n                ('multiniomialNB', MultinomialNB())])","text/html":"<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n                 CountVectorizer(stop_words={&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;after&#x27;,\n                                             &#x27;again&#x27;, &#x27;against&#x27;, &#x27;ain&#x27;, &#x27;all&#x27;,\n                                             &#x27;am&#x27;, &#x27;an&#x27;, &#x27;and&#x27;, &#x27;any&#x27;, &#x27;are&#x27;,\n                                             &#x27;aren&#x27;, &quot;aren&#x27;t&quot;, &#x27;as&#x27;, &#x27;at&#x27;, &#x27;be&#x27;,\n                                             &#x27;because&#x27;, &#x27;been&#x27;, &#x27;before&#x27;,\n                                             &#x27;being&#x27;, &#x27;below&#x27;, &#x27;between&#x27;,\n                                             &#x27;both&#x27;, &#x27;but&#x27;, &#x27;by&#x27;, &#x27;can&#x27;,\n                                             &#x27;couldn&#x27;, &quot;couldn&#x27;t&quot;, ...})),\n                (&#x27;multiniomialNB&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n                 CountVectorizer(stop_words={&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;after&#x27;,\n                                             &#x27;again&#x27;, &#x27;against&#x27;, &#x27;ain&#x27;, &#x27;all&#x27;,\n                                             &#x27;am&#x27;, &#x27;an&#x27;, &#x27;and&#x27;, &#x27;any&#x27;, &#x27;are&#x27;,\n                                             &#x27;aren&#x27;, &quot;aren&#x27;t&quot;, &#x27;as&#x27;, &#x27;at&#x27;, &#x27;be&#x27;,\n                                             &#x27;because&#x27;, &#x27;been&#x27;, &#x27;before&#x27;,\n                                             &#x27;being&#x27;, &#x27;below&#x27;, &#x27;between&#x27;,\n                                             &#x27;both&#x27;, &#x27;but&#x27;, &#x27;by&#x27;, &#x27;can&#x27;,\n                                             &#x27;couldn&#x27;, &quot;couldn&#x27;t&quot;, ...})),\n                (&#x27;multiniomialNB&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words={&#x27;a&#x27;, &#x27;about&#x27;, &#x27;above&#x27;, &#x27;after&#x27;, &#x27;again&#x27;, &#x27;against&#x27;,\n                            &#x27;ain&#x27;, &#x27;all&#x27;, &#x27;am&#x27;, &#x27;an&#x27;, &#x27;and&#x27;, &#x27;any&#x27;, &#x27;are&#x27;,\n                            &#x27;aren&#x27;, &quot;aren&#x27;t&quot;, &#x27;as&#x27;, &#x27;at&#x27;, &#x27;be&#x27;, &#x27;because&#x27;,\n                            &#x27;been&#x27;, &#x27;before&#x27;, &#x27;being&#x27;, &#x27;below&#x27;, &#x27;between&#x27;,\n                            &#x27;both&#x27;, &#x27;but&#x27;, &#x27;by&#x27;, &#x27;can&#x27;, &#x27;couldn&#x27;, &quot;couldn&#x27;t&quot;, ...})</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"pred_train_without_stopwords = pipeline_without_stopwords.predict(X_train)\npred_test_without_stopwords = pipeline_without_stopwords.predict(X_test)","metadata":{"cell_id":"7456d928602248d69506e117aa241c65","source_hash":"69611f2e","execution_start":1680109949135,"execution_millis":18170,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":43},{"cell_type":"code","source":"print(f\"Our Naive Bayes accuracy with stopwords ignored : train {accuracy_score(y_train, pred_train_without_stopwords) * 100}% - test {accuracy_score(y_test, pred_test_without_stopwords) * 100}%\")","metadata":{"cell_id":"a350ef1f48f149079e491740db88a5d4","source_hash":"18e3f308","execution_start":1680109967305,"execution_millis":10,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Our Naive Bayes accuracy with stopwords ignored : train 91.36% - test 82.464%\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"The outcome is almost the same than with our own Naive Bayes classifier.","metadata":{"cell_id":"142fdef345c04e74929c7e317e1f3407","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"9. Play with scikit-learn's version parameters. For example, see if you can consider unigram and bigram instead of only unigrams.","metadata":{"cell_id":"db1027b3f4c84987a1de81f387486ed7","deepnote_cell_type":"markdown"}},{"cell_type":"code","source":"def test_parameters(message: str, vectorizer_params: dict, multinomial_params: dict) -> None:\n    \"\"\"\n    Test the accuracy of the model with the given parameters\n    :param message: Message to print before the accuracy\n    :param vectorizer_params: Parameters of the CountVectorizer\n    :param multinomial_params: Parameters of the MultinomialNB\n    \"\"\"\n    pipeline_experimental = Pipeline([\n        ('vectorizer', CountVectorizer(**vectorizer_params)),\n        ('multiniomialNB', MultinomialNB(**multinomial_params))\n    ])\n\n    pipeline_experimental.fit(X_train, y_train)\n\n    pred_train_experimental = pipeline_experimental.predict(X_train)\n    pred_test_experimental = pipeline_experimental.predict(X_test)\n\n    print(\n        f\"{message} : train {accuracy_score(y_train, pred_train_experimental) * 100}% - test {accuracy_score(y_test, pred_test_experimental) * 100}%\")","metadata":{"cell_id":"34996150919c4b578632068f3205756b","source_hash":"e2bd6992","execution_start":1680109967321,"execution_millis":11,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":45},{"cell_type":"code","source":"test_parameters(\"Accuracy with stopwords ignored, both unigrams and bigrams\", \n                {\"stop_words\": stopswords_nltk, \"ngram_range\": (1, 2)}, {})    ","metadata":{"cell_id":"7b94e8b5a8614c4eb2df72e068302df0","source_hash":"8ec93985","execution_start":1680109967366,"execution_millis":71302,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Accuracy with stopwords ignored, both unigrams and bigrams : train 99.804% - test 85.344%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"test_parameters(\"Accuracy with stopwords ignored, only bigrams\", \n                {\"stop_words\": stopswords_nltk, \"ngram_range\": (2, 2)}, {})   ","metadata":{"cell_id":"3e7fc5a4dd1943aab484ed556647c1a2","source_hash":"5550c60f","execution_start":1680110038795,"execution_millis":61206,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Accuracy with stopwords ignored, only bigrams : train 99.964% - test 84.932%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"test_parameters(\"Accuracy with stopwords ignored, the feature are made of character n-grams.\", \n                {\"stop_words\": stopswords_nltk, \"analyzer\": \"char\"}, {})   ","metadata":{"cell_id":"84776f85f47441deb950da40ca5a68d5","source_hash":"3a8811cc","execution_start":1680110100002,"execution_millis":38068,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Accuracy with stopwords ignored, the feature are made of character n-grams. : train 61.007999999999996% - test 60.995999999999995%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"test_parameters(\"Accuracy with stopwords ignored, creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space from scikit learm documentation\", \n                {\"stop_words\": stopswords_nltk, \"analyzer\": \"char_wb\"}, {})   ","metadata":{"cell_id":"729fadc62d5e4a588ab91a02ed05ba75","source_hash":"b8253744","execution_start":1680110138090,"execution_millis":101808,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Accuracy with stopwords ignored, creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space from scikit learm documentation : train 60.748000000000005% - test 60.736000000000004%\n","output_type":"stream"}],"execution_count":49},{"cell_type":"markdown","source":"Changing the analyzer doesn't seem to improve the accuracy. Using only bigrams or (bigrams and unigrams) seems to improve the result, but mostly on the train dataset. The model is probably overfitting.","metadata":{"cell_id":"4b6067214aad4303bc2c0a9c3fda72e9","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Stemming and Lemmatization","metadata":{"cell_id":"669bb54d911f43808a98834ae640fb2a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-h2"}},{"cell_type":"markdown","source":"(2 points) Add stemming or lemmatization to your pretreatment.","metadata":{"cell_id":"87537a05d5844c389a021067dea74802","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"We chose to remove stopwords, thanks to the better results from the previous experiment.","metadata":{"cell_id":"9fc4a22f-6b43-4166-bd8b-1f5ac7424499","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"from nltk import SnowballStemmer\n\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\nanalyzer = CountVectorizer().build_analyzer()\n\ndef stemmed_words(doc):\n    return (stemmer.stem(w) for w in analyzer(doc)) ","metadata":{"cell_id":"e5042b4bd8664aeba4379a843ccbfe26","source_hash":"7e4ab562","execution_start":1680110239901,"execution_millis":4,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":"We import the SnowballStemmer from nltk and integrate it into CountVectorizer.","metadata":{"cell_id":"16d24830b0f64fc8b9473ec37991f0c5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"(1 point) Train and evaluate your model again with these pretreatment.","metadata":{"cell_id":"053c5b47-944b-4637-bdf2-e1bf0b83e77b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"pipeline_with_stemmer = Pipeline([\n    ('vectorizer', CountVectorizer(analyzer=stemmed_words)), # An analyzer is added to CountVectorizer and it performs stemming\n    ('multiniomialNB', MultinomialNB())\n])","metadata":{"cell_id":"febbe71e902749d58375d51435ac5045","source_hash":"ec94506d","execution_start":1680110239909,"execution_millis":21,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":51},{"cell_type":"code","source":"pipeline_with_stemmer.fit(X_train, y_train)","metadata":{"cell_id":"851126224c3a4024bad924d3573eb2cd","source_hash":"b2e4edca","execution_start":1680110240099,"execution_millis":99611,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"Pipeline(steps=[('vectorizer',\n                 CountVectorizer(analyzer=<function stemmed_words at 0x7f615e6dbee0>)),\n                ('multiniomialNB', MultinomialNB())])","text/html":"<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n                 CountVectorizer(analyzer=&lt;function stemmed_words at 0x7f615e6dbee0&gt;)),\n                (&#x27;multiniomialNB&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;,\n                 CountVectorizer(analyzer=&lt;function stemmed_words at 0x7f615e6dbee0&gt;)),\n                (&#x27;multiniomialNB&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function stemmed_words at 0x7f615e6dbee0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"pred_train_with_stemmer = pipeline_with_stemmer.predict(X_train)","metadata":{"cell_id":"29c2991056b04e58a6b05cddf26b3d5c","source_hash":"d406629c","execution_start":1680110339791,"execution_millis":105697,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":53},{"cell_type":"code","source":"pred_test_with_stemmer = pipeline_with_stemmer.predict(X_test)","metadata":{"cell_id":"5a70ac6f64714eb6b0d554b5abd5679c","source_hash":"5de5018f","execution_start":1680110445504,"execution_millis":96898,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[],"execution_count":54},{"cell_type":"code","source":"print(f\"Our Naive Bayes accuracy with stopwords ignored : train {accuracy_score(y_train, pred_train_with_stemmer) * 100}% - test {accuracy_score(y_test, pred_test_with_stemmer) * 100}%\")","metadata":{"cell_id":"697a590c8e7c42aea65935dfd0df423e","source_hash":"37f1db62","execution_start":1680110542435,"execution_millis":1,"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"Our Naive Bayes accuracy with stopwords ignored : train 88.444% - test 80.55600000000001%\n","output_type":"stream"}],"execution_count":55},{"cell_type":"markdown","source":"Result are worst considering that we removed stopwords.","metadata":{"cell_id":"76fa1c26a2974177816d2b6dc56303a1","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"(1 point) Are the results better or worse? Try explaining why the accuracy changed.","metadata":{"cell_id":"e7aa3883c4b0442aaebc7200cc05dd1b","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"Accuracy is better without stemming. Stemming is prone to mistakes (organization > organ ) and doesn't work well with irregular forms (was > wa).","metadata":{"cell_id":"78024c295ba04bddb29e9094e28e097a","is_collapsed":false,"formattedRanges":[],"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=872d72c8-1d54-4f60-8556-02f9978ad2c8' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.10.4","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"orig_nbformat":4,"deepnote_full_width":true,"deepnote_notebook_id":"c11b6ebfe88c428a9040f672a86f6349","deepnote_execution_queue":[]}}